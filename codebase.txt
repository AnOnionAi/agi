# pytest cache directory #

This directory contains data from the pytest's cache plugin,
which provides the `--lf` and `--ff` options, as well as the `cache` fixture.

**Do not** commit this to version control.

See [the docs](https://docs.pytest.org/en/stable/how-to/cache.html) for more information.
import os
import sys
from google.cloud import storage
from google.api_core.exceptions import GoogleAPIError

def upload_directory_to_gcs(local_directory, bucket_name, gcs_prefix='agi/'):
    """
    Uploads the contents of a local directory to a Google Cloud Storage bucket.

    :param local_directory: Path to the local directory to upload.
    :param bucket_name: Name of the GCS bucket.
    :param gcs_prefix: GCS prefix (folder path) within the bucket.
    """
    # Initialize the GCS client
    try:
        client = storage.Client()
    except Exception as e:
        print(f"Error initializing Google Cloud Storage client: {e}")
        sys.exit(1)

    # Get the bucket
    try:
        bucket = client.get_bucket(bucket_name)
    except GoogleAPIError as e:
        print(f"Error accessing bucket '{bucket_name}': {e}")
        sys.exit(1)
    except Exception as e:
        print(f"Unexpected error accessing bucket '{bucket_name}': {e}")
        sys.exit(1)

    # Walk through the local directory
    for root, dirs, files in os.walk(local_directory):
        for filename in files:
            local_path = os.path.join(root, filename)

            # Compute the relative path to maintain directory structure in GCS
            relative_path = os.path.relpath(local_path, local_directory)
            gcs_path = os.path.join(gcs_prefix, relative_path).replace("\\", "/")  # For Windows compatibility

            # Create a blob object
            blob = bucket.blob(gcs_path)

            # Upload the file
            try:
                blob.upload_from_filename(local_path)
                print(f"Uploaded {local_path} to gs://{bucket_name}/{gcs_path}")
            except FileNotFoundError:
                print(f"Error: The file {local_path} was not found.")
            except GoogleAPIError as e:
                print(f"Google API error uploading {local_path}: {e}")
            except Exception as e:
                print(f"Unexpected error uploading {local_path}: {e}")

    print("All files have been uploaded successfully.")

if __name__ == "__main__":
    import argparse

    # Set up command-line argument parsing
    parser = argparse.ArgumentParser(description='Upload a local directory to a Google Cloud Storage bucket.')
    parser.add_argument('--local-dir', type=str, default='data',
                        help='Path to the local directory to upload. Default is "data".')
    parser.add_argument('--bucket', type=str, required=True,
                        help='Name of the GCS bucket to upload to.')
    parser.add_argument('--prefix', type=str, default='agi/',
                        help='GCS prefix (folder path) within the bucket. Default is "agi/".')

    args = parser.parse_args()

    # Validate the local directory
    if not os.path.isdir(args.local_dir):
        print(f"Error: The directory '{args.local_dir}' does not exist.")
        sys.exit(1)

    # Call the upload function
    upload_directory_to_gcs(args.local_dir, args.bucket, args.prefix)
[tool.poetry]
name = "agi"
version = "0.2.0"
description = "AGI: Toward General AI through Advanced Programming Language Understanding"
authors = ["Matthew <matthew@zeti.dev>"]
readme = "README.md"

[tool.poetry.dependencies]
python = "^3.11"
tiktoken = "^0.5.2"
torch = "^2.1.1"
argparse = "^1.4.0"
lightning = "^2.1.2"
tensorboard = "^2.15.1"
pyyaml = "^6.0.1"
wandb = "^0.16.1"
google-cloud-storage = "^2.18.2"

[build-system]
requires = ["poetry-core"]
build-backend = "poetry.core.masonry.api"
import torch

def create_mask(mask, batch_size, heads, current_seq_length):
    mask = mask.unsqueeze(1)  # Now [batch_size, 1, seq_len]
    mask = mask.repeat(1, heads, 1)  # Now [batch_size, num_heads, seq_len]
    mask = mask.view(batch_size * heads, 1, current_seq_length)  # Now [batch_size*num_heads, 1, seq_len]
    mask = mask.repeat(1, current_seq_length, 1)  # Now [batch_size*num_heads, seq_len, seq_len]
    return mask

def test_create_mask():
    batch_size = 2
    heads = 4
    seq_length = 5
    mask = torch.ones(batch_size, seq_length)
    output_mask = create_mask(mask, batch_size, heads, seq_length)
    assert output_mask.shape == (batch_size * heads, seq_length, seq_length)

# Running the test
test_create_mask()
import pytest
import torch
from predict import top_p_filtering  # Import your top_p_filtering function

# Fixture for creating mock logits
@pytest.fixture
def mock_logits():
    return torch.randn(10)  # A tensor with random logits

# Test for top_p_filtering with top_p = 0.9
def test_top_p_filtering_90(mock_logits):
    filtered_logits = top_p_filtering(mock_logits, top_p=0.9)
    assert torch.is_tensor(filtered_logits), "Output should be a tensor"
    assert not torch.any(filtered_logits == -float('Inf')), "No logits should be set to -Inf with top_p=0.9"

# Test for top_p_filtering with top_p = 0.5
def test_top_p_filtering_50(mock_logits):
    filtered_logits = top_p_filtering(mock_logits, top_p=0.5)
    assert torch.is_tensor(filtered_logits), "Output should be a tensor"
    assert torch.any(filtered_logits == -float('Inf')), "Some logits should be set to -Inf with top_p=0.5"

Welcome to 'AGI' – our ambitious journey towards creating a scaled-down yet powerful AI model. Inspired by the sophisticated capabilities of GPT-like models, this project focuses on harnessing deep learning, specifically using PyTorch and Transformer architectures, to develop an AI adept in programming language comprehension and generation.

Our mission is to engineer an AI model that can not only understand and write code in multiple programming languages but also explain its reasoning and logic. The immediate scope of our project centers on Python, with plans to expand into other languages as we progress.

Insert Code Into Text File:

`find . -type f \( -name "*.py" -o -name "*.toml" -o -name "*.yaml" -o -name "*.ipynb" -o -name "*.md" \) -exec cat {} + > codebase.txt`

`find . -type f \( -name "*.py" -o -name "*.toml" -o -name "*.yaml" -o -name "*.md" \) -exec cat {} + > codebase.txt && \
find . -type f -name "*.ipynb" -exec sh -c 'jq -r ".cells[].source[]" "$0"' {} \; >> codebase.txt`


Upload

`export GOOGLE_APPLICATION_CREDENTIALS="zeti-nube-dev-key.json"`

Encode:

`poetry run python src/main.py encode data/raw_data.txt data/training_data.txt`

Train:

`poetry run python src/main.py train`

Predict:

`poetry run python src/main.py predict`

Concat the project files so that the project can be seen by the Ai. 
```cat src/*.py > codebase.py```

View Tensorboard
```tensorboard --logdir=tb_logs --bind_all```

Key Features of the Project:

Transformer-Based Model: Leveraging the power of the Transformer architecture to process and generate programming code.
PyTorch Framework: Utilizing the flexibility and robustness of PyTorch for model development and training.
Focus on Python: Initial emphasis on mastering Python, given its prominence and versatility in programming and AI.
Scalable Design: Building the model with scalability in mind, optimized for training on Google Colab's A100 GPUs.
Future Expansion: Plans to incorporate more programming languages and potentially develop a web-based UI for interacting with the model.
Project Roadmap:

Project Setup: Establishing a solid foundation with a well-structured repository and clear coding standards.
Model Architecture Design: Crafting a Transformer model tailored to understand programming languages.
Data Collection & Processing: Amassing a diverse dataset of code examples and documentation for training.
Model Training & Evaluation: Rigorously training and fine-tuning the model to ensure accuracy and efficiency.
Application Development: Building interfaces and applications to demonstrate and utilize the model's capabilities.
Continuous Improvement: Iterative enhancements and expansion into other languages and functionalities.
Contributions & Collaboration:
We welcome contributions, suggestions, and collaborations from enthusiasts and experts alike. Whether you're a seasoned AI researcher, a programming language guru, or just passionate about AI and coding, your input can help shape the future of this project.

Let’s embark on this exciting journey to push the boundaries of AI in programming language understanding and generation!
batch_size: 32
dropout_rate: 0.1
embed_size: 768
forward_expansion: 4
heads: 16
max_epochs: 1
num_layers: 24
sequence_length: 256
training_file_path: data/svelte_docs/training_data.txt
validation_file_path: data/svelte_docs/validation_data.txt
vocab_size: 50233
batch_size: 32
dropout_rate: 0.1
embed_size: 1024
forward_expansion: 4
heads: 16
max_epochs: 1
num_layers: 24
sequence_length: 512
training_file_path: data/svelte_docs/training_data.txt
validation_file_path: data/svelte_docs/validation_data.txt
vocab_size: 50233
batch_size: 32
dropout_rate: 0.1
embed_size: 768
forward_expansion: 4
heads: 16
max_epochs: 1
num_layers: 12
sequence_length: 128
training_file_path: data/svelte_docs/training_data.txt
validation_file_path: data/svelte_docs/validation_data.txt
vocab_size: 50233
batch_size: 32
dropout_rate: 0.1
embed_size: 1024
forward_expansion: 4
heads: 16
max_epochs: 100
num_layers: 24
sequence_length: 64
trainable_pos_emb: false
training_file_path: data/training_data.txt
validation_file_path: data/validation_data.txt
vocab_size: 50233
batch_size: 32
dropout_rate: 0.1
embed_size: 1024
forward_expansion: 4
heads: 16
max_epochs: 100
num_layers: 24
sequence_length: 64
trainable_pos_emb: false
training_file_path: data/training_data.txt
validation_file_path: data/validation_data.txt
vocab_size: 50233
batch_size: 32
dropout_rate: 0.1
embed_size: 768
forward_expansion: 4
heads: 16
max_epochs: 1
num_layers: 12
sequence_length: 128
training_file_path: data/svelte_docs/training_data.txt
validation_file_path: data/svelte_docs/validation_data.txt
vocab_size: 50233
batch_size: 32
dropout_rate: 0.1
embed_size: 1024
forward_expansion: 4
heads: 16
max_epochs: 1
num_layers: 24
sequence_length: 1024
training_file_path: data/svelte_docs/training_data.txt
validation_file_path: data/svelte_docs/validation_data.txt
vocab_size: 50233
batch_size: 32
dropout_rate: 0.1
embed_size: 768
forward_expansion: 4
heads: 16
max_epochs: 1
num_layers: 12
sequence_length: 128
training_file_path: data/svelte_docs/training_data.txt
validation_file_path: data/svelte_docs/validation_data.txt
vocab_size: 50233
batch_size: 32
dropout_rate: 0.1
embed_size: 768
forward_expansion: 4
heads: 16
max_epochs: 1
num_layers: 12
sequence_length: 128
training_file_path: data/svelte_docs/training_data.txt
validation_file_path: data/svelte_docs/validation_data.txt
vocab_size: 50233
batch_size: 32
dropout_rate: 0.1
embed_size: 768
forward_expansion: 4
heads: 16
max_epochs: 1
num_layers: 12
sequence_length: 128
training_file_path: data/svelte_docs/training_data.txt
validation_file_path: data/svelte_docs/validation_data.txt
vocab_size: 50233
batch_size: 32
dropout_rate: 0.1
embed_size: 1024
forward_expansion: 4
heads: 16
max_epochs: 1
num_layers: 24
sequence_length: 512
training_file_path: data/svelte_docs/training_data.txt
validation_file_path: data/svelte_docs/validation_data.txt
vocab_size: 50233
batch_size: 32
dropout_rate: 0.1
embed_size: 768
forward_expansion: 4
heads: 16
max_epochs: 1
num_layers: 24
sequence_length: 256
training_file_path: data/svelte_docs/training_data.txt
validation_file_path: data/svelte_docs/validation_data.txt
vocab_size: 50233
batch_size: 32
dropout_rate: 0.1
embed_size: 1024
forward_expansion: 4
heads: 16
max_epochs: 10
num_layers: 24
sequence_length: 64
trainable_pos_emb: false
training_file_path: data/training_data.txt
validation_file_path: data/validation_data.txt
vocab_size: 50233
batch_size: 32
dropout_rate: 0.1
embed_size: 1024
forward_expansion: 4
heads: 16
max_epochs: 1
num_layers: 24
sequence_length: 64
trainable_pos_emb: false
training_file_path: data/training_data.txt
validation_file_path: data/validation_data.txt
vocab_size: 50233
batch_size: 32
dropout_rate: 0.1
embed_size: 1024
forward_expansion: 4
heads: 16
max_epochs: 1
num_layers: 24
sequence_length: 64
trainable_pos_emb: false
training_file_path: data/training_data.txt
validation_file_path: data/validation_data.txt
vocab_size: 50233
batch_size: 32
dropout_rate: 0.1
embed_size: 1024
forward_expansion: 4
heads: 16
max_epochs: 1
num_layers: 24
sequence_length: 64
trainable_pos_emb: false
training_file_path: data/training_data.txt
validation_file_path: data/validation_data.txt
vocab_size: 50233
batch_size: 32
dropout_rate: 0.1
embed_size: 1024
forward_expansion: 4
heads: 16
max_epochs: 1
num_layers: 24
sequence_length: 1024
training_file_path: data/svelte_docs/training_data.txt
validation_file_path: data/svelte_docs/validation_data.txt
vocab_size: 50233
batch_size: 32
dropout_rate: 0.1
embed_size: 768
forward_expansion: 4
heads: 16
max_epochs: 1
num_layers: 12
sequence_length: 256
training_file_path: data/svelte_docs/training_data.txt
validation_file_path: data/svelte_docs/validation_data.txt
vocab_size: 50233
batch_size: 32
dropout_rate: 0.1
embed_size: 1024
forward_expansion: 4
heads: 16
max_epochs: 1
num_layers: 12
sequence_length: 128
trainable_pos_emb: false
training_file_path: data/training_data.txt
validation_file_path: data/validation_data.txt
vocab_size: 50233
batch_size: 32
dropout_rate: 0.1
embed_size: 1024
forward_expansion: 4
heads: 16
max_epochs: 1
num_layers: 12
sequence_length: 128
training_file_path: data/training_data.txt
validation_file_path: data/validation_data.txt
vocab_size: 50233
batch_size: 32
dropout_rate: 0.1
embed_size: 1024
forward_expansion: 4
heads: 16
max_epochs: 1
num_layers: 24
sequence_length: 64
training_file_path: data/svelte_docs/training_data.txt
validation_file_path: data/svelte_docs/validation_data.txt
vocab_size: 50233
batch_size: 32
dropout_rate: 0.1
embed_size: 1024
forward_expansion: 4
heads: 16
max_epochs: 1
num_layers: 12
sequence_length: 128
trainable_pos_emb: false
training_file_path: data/training_data.txt
validation_file_path: data/validation_data.txt
vocab_size: 50233
batch_size: 32
dropout_rate: 0.1
embed_size: 1024
forward_expansion: 4
heads: 16
max_epochs: 1
num_layers: 12
sequence_length: 128
trainable_pos_emb: false
training_file_path: data/training_data.txt
validation_file_path: data/validation_data.txt
vocab_size: 50233
wandb_version: 1

_wandb:
  desc: null
  value:
    python_version: 3.11.4
    cli_version: 0.16.1
    framework: lightning
    is_jupyter_run: false
    is_kaggle_kernel: false
    start_time: 1703009072.249887
    t:
      1:
      - 1
      - 9
      - 55
      - 103
      2:
      - 1
      - 9
      - 55
      - 103
      3:
      - 7
      - 19
      - 23
      4: 3.11.4
      5: 0.16.1
      8:
      - 4
      - 5
      13: darwin-arm64
    m:
    - 1: trainer/global_step
      6:
      - 3
batch_size:
  desc: null
  value: 32
embed_size:
  desc: null
  value: 1024
num_layers:
  desc: null
  value: 24
heads:
  desc: null
  value: 16
forward_expansion:
  desc: null
  value: 4
dropout_rate:
  desc: null
  value: 0.1
vocab_size:
  desc: null
  value: 50233
sequence_length:
  desc: null
  value: 512
max_epochs:
  desc: null
  value: 1
training_file_path:
  desc: null
  value: data/svelte_docs/training_data.txt
validation_file_path:
  desc: null
  value: data/svelte_docs/validation_data.txt
wandb_version: 1

_wandb:
  desc: null
  value:
    python_version: 3.11.4
    cli_version: 0.16.1
    framework: lightning
    is_jupyter_run: false
    is_kaggle_kernel: false
    start_time: 1703009761.536373
    t:
      1:
      - 1
      - 9
      - 55
      - 103
      2:
      - 1
      - 9
      - 55
      - 103
      3:
      - 2
      - 7
      - 19
      - 23
      4: 3.11.4
      5: 0.16.1
      8:
      - 4
      - 5
      13: darwin-arm64
    m:
    - 1: trainer/global_step
      6:
      - 3
batch_size:
  desc: null
  value: 32
embed_size:
  desc: null
  value: 768
num_layers:
  desc: null
  value: 12
heads:
  desc: null
  value: 16
forward_expansion:
  desc: null
  value: 4
dropout_rate:
  desc: null
  value: 0.1
vocab_size:
  desc: null
  value: 50233
sequence_length:
  desc: null
  value: 256
max_epochs:
  desc: null
  value: 1
training_file_path:
  desc: null
  value: data/svelte_docs/training_data.txt
validation_file_path:
  desc: null
  value: data/svelte_docs/validation_data.txt
wandb_version: 1

_wandb:
  desc: null
  value:
    python_version: 3.11.4
    cli_version: 0.16.1
    framework: lightning
    is_jupyter_run: false
    is_kaggle_kernel: false
    start_time: 1703022924.030229
    t:
      1:
      - 1
      - 9
      - 55
      - 103
      2:
      - 1
      - 9
      - 55
      - 103
      3:
      - 7
      - 19
      - 23
      4: 3.11.4
      5: 0.16.1
      8:
      - 4
      - 5
      13: darwin-arm64
    m:
    - 1: trainer/global_step
      6:
      - 3
batch_size:
  desc: null
  value: 32
embed_size:
  desc: null
  value: 768
num_layers:
  desc: null
  value: 12
heads:
  desc: null
  value: 16
forward_expansion:
  desc: null
  value: 4
dropout_rate:
  desc: null
  value: 0.1
vocab_size:
  desc: null
  value: 50233
sequence_length:
  desc: null
  value: 128
max_epochs:
  desc: null
  value: 1
training_file_path:
  desc: null
  value: data/svelte_docs/training_data.txt
validation_file_path:
  desc: null
  value: data/svelte_docs/validation_data.txt
wandb_version: 1

_wandb:
  desc: null
  value:
    python_version: 3.11.4
    cli_version: 0.16.1
    framework: lightning
    is_jupyter_run: false
    is_kaggle_kernel: false
    start_time: 1703008185.205721
    t:
      1:
      - 1
      - 9
      - 55
      - 103
      2:
      - 1
      - 9
      - 55
      - 103
      3:
      - 7
      - 19
      - 23
      4: 3.11.4
      5: 0.16.1
      8:
      - 4
      - 5
      13: darwin-arm64
    m:
    - 1: trainer/global_step
      6:
      - 3
batch_size:
  desc: null
  value: 32
embed_size:
  desc: null
  value: 1024
num_layers:
  desc: null
  value: 24
heads:
  desc: null
  value: 16
forward_expansion:
  desc: null
  value: 4
dropout_rate:
  desc: null
  value: 0.1
vocab_size:
  desc: null
  value: 50233
sequence_length:
  desc: null
  value: 1024
max_epochs:
  desc: null
  value: 1
training_file_path:
  desc: null
  value: data/svelte_docs/training_data.txt
validation_file_path:
  desc: null
  value: data/svelte_docs/validation_data.txt
wandb_version: 1

_wandb:
  desc: null
  value:
    python_version: 3.11.4
    cli_version: 0.16.1
    framework: lightning
    is_jupyter_run: false
    is_kaggle_kernel: false
    start_time: 1703008358.252127
    t:
      1:
      - 1
      - 9
      - 55
      - 103
      2:
      - 1
      - 9
      - 55
      - 103
      3:
      - 7
      - 19
      - 23
      4: 3.11.4
      5: 0.16.1
      8:
      - 4
      - 5
      13: darwin-arm64
    m:
    - 1: trainer/global_step
      6:
      - 3
batch_size:
  desc: null
  value: 32
embed_size:
  desc: null
  value: 1024
num_layers:
  desc: null
  value: 24
heads:
  desc: null
  value: 16
forward_expansion:
  desc: null
  value: 4
dropout_rate:
  desc: null
  value: 0.1
vocab_size:
  desc: null
  value: 50233
sequence_length:
  desc: null
  value: 512
max_epochs:
  desc: null
  value: 1
training_file_path:
  desc: null
  value: data/svelte_docs/training_data.txt
validation_file_path:
  desc: null
  value: data/svelte_docs/validation_data.txt
wandb_version: 1

_wandb:
  desc: null
  value:
    python_version: 3.11.4
    cli_version: 0.16.1
    framework: lightning
    is_jupyter_run: false
    is_kaggle_kernel: false
    start_time: 1703042921.65427
    t:
      1:
      - 1
      - 9
      - 55
      - 103
      2:
      - 1
      - 9
      - 55
      - 103
      3:
      - 2
      - 7
      - 19
      - 23
      4: 3.11.4
      5: 0.16.1
      8:
      - 4
      - 5
      13: darwin-arm64
    m:
    - 1: trainer/global_step
      6:
      - 3
batch_size:
  desc: null
  value: 32
embed_size:
  desc: null
  value: 1024
num_layers:
  desc: null
  value: 24
heads:
  desc: null
  value: 16
forward_expansion:
  desc: null
  value: 4
dropout_rate:
  desc: null
  value: 0.1
vocab_size:
  desc: null
  value: 50233
sequence_length:
  desc: null
  value: 64
max_epochs:
  desc: null
  value: 1
training_file_path:
  desc: null
  value: data/svelte_docs/training_data.txt
validation_file_path:
  desc: null
  value: data/svelte_docs/validation_data.txt
wandb_version: 1

_wandb:
  desc: null
  value:
    python_version: 3.11.4
    cli_version: 0.16.1
    framework: lightning
    is_jupyter_run: false
    is_kaggle_kernel: false
    start_time: 1703010642.21774
    t:
      1:
      - 1
      - 9
      - 55
      - 103
      2:
      - 1
      - 9
      - 55
      - 103
      3:
      - 7
      - 19
      - 23
      4: 3.11.4
      5: 0.16.1
      8:
      - 4
      - 5
      13: darwin-arm64
    m:
    - 1: trainer/global_step
      6:
      - 3
batch_size:
  desc: null
  value: 32
embed_size:
  desc: null
  value: 1024
num_layers:
  desc: null
  value: 24
heads:
  desc: null
  value: 16
forward_expansion:
  desc: null
  value: 4
dropout_rate:
  desc: null
  value: 0.1
vocab_size:
  desc: null
  value: 50233
sequence_length:
  desc: null
  value: 1024
max_epochs:
  desc: null
  value: 1
training_file_path:
  desc: null
  value: data/svelte_docs/training_data.txt
validation_file_path:
  desc: null
  value: data/svelte_docs/validation_data.txt
wandb_version: 1

_wandb:
  desc: null
  value:
    python_version: 3.11.4
    cli_version: 0.16.1
    framework: lightning
    is_jupyter_run: false
    is_kaggle_kernel: false
    start_time: 1703019987.47534
    t:
      1:
      - 1
      - 9
      - 55
      - 103
      2:
      - 1
      - 9
      - 55
      - 103
      3:
      - 2
      - 7
      - 19
      - 23
      4: 3.11.4
      5: 0.16.1
      8:
      - 4
      - 5
      13: darwin-arm64
    m:
    - 1: trainer/global_step
      6:
      - 3
    - 1: train_loss_step
      5: 1
      6:
      - 1
    - 1: epoch
      5: 1
      6:
      - 1
batch_size:
  desc: null
  value: 32
embed_size:
  desc: null
  value: 768
num_layers:
  desc: null
  value: 12
heads:
  desc: null
  value: 16
forward_expansion:
  desc: null
  value: 4
dropout_rate:
  desc: null
  value: 0.1
vocab_size:
  desc: null
  value: 50233
sequence_length:
  desc: null
  value: 128
max_epochs:
  desc: null
  value: 1
training_file_path:
  desc: null
  value: data/svelte_docs/training_data.txt
validation_file_path:
  desc: null
  value: data/svelte_docs/validation_data.txt
wandb_version: 1

_wandb:
  desc: null
  value:
    python_version: 3.11.4
    cli_version: 0.16.1
    framework: lightning
    is_jupyter_run: false
    is_kaggle_kernel: false
    start_time: 1703019845.51426
    t:
      1:
      - 1
      - 9
      - 55
      - 103
      2:
      - 1
      - 9
      - 55
      - 103
      3:
      - 7
      - 19
      - 23
      4: 3.11.4
      5: 0.16.1
      8:
      - 5
      13: darwin-arm64
    m:
    - 1: trainer/global_step
      6:
      - 3
batch_size:
  desc: null
  value: 32
embed_size:
  desc: null
  value: 768
num_layers:
  desc: null
  value: 24
heads:
  desc: null
  value: 16
forward_expansion:
  desc: null
  value: 4
dropout_rate:
  desc: null
  value: 0.1
vocab_size:
  desc: null
  value: 50233
sequence_length:
  desc: null
  value: 256
max_epochs:
  desc: null
  value: 1
training_file_path:
  desc: null
  value: data/svelte_docs/training_data.txt
validation_file_path:
  desc: null
  value: data/svelte_docs/validation_data.txt
wandb_version: 1

_wandb:
  desc: null
  value:
    python_version: 3.11.4
    cli_version: 0.16.1
    framework: lightning
    is_jupyter_run: false
    is_kaggle_kernel: false
    start_time: 1703023306.755286
    t:
      1:
      - 1
      - 9
      - 55
      - 103
      2:
      - 1
      - 9
      - 55
      - 103
      3:
      - 2
      - 7
      - 19
      - 23
      4: 3.11.4
      5: 0.16.1
      8:
      - 4
      - 5
      13: darwin-arm64
    m:
    - 1: trainer/global_step
      6:
      - 3
    - 1: train_loss_step
      5: 1
      6:
      - 1
    - 1: epoch
      5: 1
      6:
      - 1
    - 1: val_loss
      5: 1
      6:
      - 1
    - 1: train_loss_epoch
      5: 1
      6:
      - 1
batch_size:
  desc: null
  value: 32
embed_size:
  desc: null
  value: 768
num_layers:
  desc: null
  value: 12
heads:
  desc: null
  value: 16
forward_expansion:
  desc: null
  value: 4
dropout_rate:
  desc: null
  value: 0.1
vocab_size:
  desc: null
  value: 50233
sequence_length:
  desc: null
  value: 128
max_epochs:
  desc: null
  value: 1
training_file_path:
  desc: null
  value: data/svelte_docs/training_data.txt
validation_file_path:
  desc: null
  value: data/svelte_docs/validation_data.txt
wandb_version: 1

_wandb:
  desc: null
  value:
    python_version: 3.11.4
    cli_version: 0.16.1
    framework: lightning
    is_jupyter_run: false
    is_kaggle_kernel: false
    start_time: 1703019714.930401
    t:
      1:
      - 1
      - 9
      - 55
      - 103
      2:
      - 1
      - 9
      - 55
      - 103
      3:
      - 7
      - 19
      - 23
      4: 3.11.4
      5: 0.16.1
      8:
      - 4
      - 5
      13: darwin-arm64
    m:
    - 1: trainer/global_step
      6:
      - 3
batch_size:
  desc: null
  value: 32
embed_size:
  desc: null
  value: 768
num_layers:
  desc: null
  value: 24
heads:
  desc: null
  value: 16
forward_expansion:
  desc: null
  value: 4
dropout_rate:
  desc: null
  value: 0.1
vocab_size:
  desc: null
  value: 50233
sequence_length:
  desc: null
  value: 256
max_epochs:
  desc: null
  value: 1
training_file_path:
  desc: null
  value: data/svelte_docs/training_data.txt
validation_file_path:
  desc: null
  value: data/svelte_docs/validation_data.txt
wandb_version: 1

_wandb:
  desc: null
  value:
    python_version: 3.11.4
    cli_version: 0.16.1
    framework: lightning
    is_jupyter_run: false
    is_kaggle_kernel: false
    start_time: 1703020434.160484
    t:
      1:
      - 1
      - 9
      - 55
      - 103
      2:
      - 1
      - 9
      - 55
      - 103
      3:
      - 2
      - 7
      - 19
      - 23
      4: 3.11.4
      5: 0.16.1
      8:
      - 4
      - 5
      13: darwin-arm64
    m:
    - 1: trainer/global_step
      6:
      - 3
batch_size:
  desc: null
  value: 32
embed_size:
  desc: null
  value: 768
num_layers:
  desc: null
  value: 12
heads:
  desc: null
  value: 16
forward_expansion:
  desc: null
  value: 4
dropout_rate:
  desc: null
  value: 0.1
vocab_size:
  desc: null
  value: 50233
sequence_length:
  desc: null
  value: 128
max_epochs:
  desc: null
  value: 1
training_file_path:
  desc: null
  value: data/svelte_docs/training_data.txt
validation_file_path:
  desc: null
  value: data/svelte_docs/validation_data.txt
wandb_version: 1

_wandb:
  desc: null
  value:
    python_version: 3.11.4
    cli_version: 0.16.1
    framework: lightning
    is_jupyter_run: false
    is_kaggle_kernel: false
    start_time: 1703020485.295726
    t:
      1:
      - 1
      - 9
      - 55
      - 103
      2:
      - 1
      - 9
      - 55
      - 103
      3:
      - 2
      - 7
      - 19
      - 23
      4: 3.11.4
      5: 0.16.1
      8:
      - 4
      - 5
      13: darwin-arm64
    m:
    - 1: trainer/global_step
      6:
      - 3
    - 1: train_loss_step
      5: 1
      6:
      - 1
    - 1: epoch
      5: 1
      6:
      - 1
    - 1: val_loss
      5: 1
      6:
      - 1
    - 1: train_loss_epoch
      5: 1
      6:
      - 1
batch_size:
  desc: null
  value: 32
embed_size:
  desc: null
  value: 768
num_layers:
  desc: null
  value: 12
heads:
  desc: null
  value: 16
forward_expansion:
  desc: null
  value: 4
dropout_rate:
  desc: null
  value: 0.1
vocab_size:
  desc: null
  value: 50233
sequence_length:
  desc: null
  value: 128
max_epochs:
  desc: null
  value: 1
training_file_path:
  desc: null
  value: data/svelte_docs/training_data.txt
validation_file_path:
  desc: null
  value: data/svelte_docs/validation_data.txt
# encode.py
import os
import json
import tiktoken
from gcs_utils import download_blob, upload_blob, list_blobs

def restructure_data(all_text, context_window):
    """
    Restructure data into chunks of a specified context window.
    """
    chunks = [all_text[i:i+context_window] for i in range(0, len(all_text), context_window)]
    structured_data = []
    for idx, chunk in enumerate(chunks, start=1):
        json_obj = {
            "id": idx,
            "text": chunk.strip(),
            "length": len(chunk.split()),  # Number of words in the chunk
            "ended": idx == len(chunks)  # True if it's the last chunk
        }
        structured_data.append(json_obj)
    return structured_data

def encode_data(bucket_name, source_blob_name, destination_folder, context_window=1024, val_ratio=0.1):
    """
    Downloads data from GCS, encodes it, and uploads the processed data back to GCS.
    """
    local_raw_file = "raw_data.txt"
    download_blob(bucket_name, source_blob_name, local_raw_file)
    
    # Read and restructure data
    with open(local_raw_file, 'r', encoding='utf-8') as file:
        all_text = file.read()
    structured_data = restructure_data(all_text, context_window)
    
    # Write structured data to a temporary local file
    structured_file = "structured_data.jsonl"
    with open(structured_file, 'w', encoding='utf-8') as file:
        for item in structured_data:
            file.write(json.dumps(item) + '\n')
    
    # Encode the structured data
    encoder = tiktoken.encoding_for_model("gpt2")
    encoded_file = "encoded_data.txt"
    with open(structured_file, 'r', encoding='utf-8') as infile, open(encoded_file, 'w', encoding='utf-8') as outfile:
        for line in infile:
            data = json.loads(line)
            tokens = encoder.encode(data['text'])
            token_str = ' '.join(map(str, tokens))
            outfile.write(token_str + '\n')
    
    # Split into training and validation sets
    train_file = "training_data.txt"
    val_file = "validation_data.txt"
    with open(encoded_file, 'r', encoding='utf-8') as infile:
        tokens = infile.readlines()
    val_size = int(len(tokens) * val_ratio)
    train_tokens = tokens[:-val_size]
    val_tokens = tokens[-val_size:]
    with open(train_file, 'w', encoding='utf-8') as outfile:
        outfile.writelines(train_tokens)
    with open(val_file, 'w', encoding='utf-8') as outfile:
        outfile.writelines(val_tokens)
    
    # Upload processed data back to GCS
    upload_blob(bucket_name, train_file, f"{destination_folder}/training_data.txt")
    upload_blob(bucket_name, val_file, f"{destination_folder}/validation_data.txt")
    print("Encoding and uploading complete.")

def encode_jsonl_file(input_file):
    # Initialize the tokenizer for GPT-2
    encoder = tiktoken.encoding_for_model("gpt2")

    # Construct output file name by replacing the extension with '.encoded.txt'
    base_name = os.path.splitext(input_file)[0]
    output_file = base_name + '.encoded.txt'

    with open(input_file, 'r', encoding='utf-8') as infile, \
         open(output_file, 'w', encoding='utf-8') as outfile:
        for line in infile:
            # Parse the JSON line
            data = json.loads(line)

            # Encode the text
            encoded_text = encoder.encode(data['text'])

            # Convert encoded tokens to a string and write to the output file
            outfile.write(' '.join(map(str, encoded_text)) + '\n')

    print(f"Encoded file saved as: {output_file}")

def encode_text_file(file_path, val_ratio=0.1, context_window=1024):
    structured_file = os.path.join(os.path.dirname(file_path), 'structured_data.txt')
    restructure_data(file_path, structured_file, context_window)

    encoder = tiktoken.encoding_for_model("gpt2")
    directory = os.path.dirname(structured_file)
    encoded_file = os.path.join(directory, 'encoded_data.txt')
    train_file = os.path.join(directory, 'training_data.txt')
    val_file = os.path.join(directory, 'validation_data.txt')

    with open(structured_file, 'r', encoding='utf-8') as file:
        sentences = file.readlines()

    with open(encoded_file, 'w', encoding='utf-8') as file:
        for sentence in sentences:
            tokens = encoder.encode(sentence.strip())
            token_str = ' '.join(map(str, tokens))
            file.write(token_str + '\n')

    format_and_split_data(encoded_file, train_file, val_file, val_ratio)

def format_and_split_data(file_path, train_file, val_file, val_ratio=0.1):
    with open(file_path, 'r', encoding='utf-8') as file:
        tokens = [' '.join(line.strip().split()) for line in file.readlines()]

    val_size = int(len(tokens) * val_ratio)
    train_tokens = tokens[:-val_size]
    val_tokens = tokens[-val_size:]

    with open(train_file, 'w', encoding='utf-8') as file:
        file.write('\n'.join(train_tokens) + '\n')
    with open(val_file, 'w', encoding='utf-8') as file:
        file.write('\n'.join(val_tokens) + '\n')

def find_vocab_size(file_path):
    max_token = 0
    with open(file_path, 'r', encoding='utf-8') as file:
        for line in file:
            tokens = [int(token) for token in line.strip().split()]
            max_token = max(max_token, max(tokens))
    return max_token + 1


if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description='Process and encode data from GCS.')
    parser.add_argument('--bucket', type=str, required=True, help='Name of the GCS bucket.')
    parser.add_argument('--source_blob', type=str, required=True, help='GCS path to the source raw data file.')
    parser.add_argument('--destination_folder', type=str, default='agi/experiments/', help='GCS folder to upload processed data.')
    parser.add_argument('--context_window', type=int, default=1024, help='Context window size for data restructuring.')
    parser.add_argument('--val_ratio', type=float, default=0.1, help='Validation data ratio.')
    
    args = parser.parse_args()
    encode_data(args.bucket, args.source_blob, args.destination_folder, args.context_window, args.val_ratio)
# util.py
import random
import math
import torch
from dataset import TokenizedTextDataset  

# Initalize with sinusoidal positional encoding but still learnable 
def sinusoidal_positional_encoding(embed_size, max_len):
    pe = torch.zeros(max_len, embed_size)
    position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
    div_term = torch.exp(torch.arange(0, embed_size, 2).float() * (-math.log(10000.0) / embed_size))
    pe[:, 0::2] = torch.sin(position * div_term)
    pe[:, 1::2] = torch.cos(position * div_term)
    return pe.unsqueeze(0)


def find_sequence_length():
    # Load the dataset
    train_dataset = TokenizedTextDataset('data/training_data.txt')

    # Ensure the sample size is not larger than the dataset size
    sample_size = min(10000, len(train_dataset))  # Adjust this value based on your needs

    # Sample a subset of the dataset
    samples = random.sample(list(train_dataset), sample_size)

    # Print out a few samples
    for i in range(5):
        print(samples[i])

    # Compute the maximum length of the samples
    max_len = max(max(tensor.shape[0] for tensor in sample) for sample in samples)
    print("Max Length", max_len)

#if __name__ == "__main__":
   # find_sequence_length()# predict.py
import torch
import torch.nn.functional as F
from torch.distributions import Categorical
import tiktoken
from gcs_utils import download_blob
from model import GPTModel
import yaml
import os
from gcs_utils import download_blob, upload_blob

def get_latest_checkpoint(bucket_name, experiments_folder):
    checkpoint_dir = f"{experiments_folder}/checkpoints/"
    blobs = list_blobs(bucket_name, prefix=checkpoint_dir)
    checkpoint_files = [blob.name for blob in blobs if blob.name.endswith('.pth')]

    if not checkpoint_files:
        raise Exception("No checkpoint files found in the specified directory.")

    # Assuming checkpoints are named with step numbers or timestamps
    latest_checkpoint = sorted(checkpoint_files, reverse=True)[0]
    local_checkpoint = "latest_model.pth"
    download_blob(bucket_name, latest_checkpoint, local_checkpoint)
    return local_checkpoint

def read_hparams(bucket_name, experiments_folder, model_version=None):
    if model_version:
        hparams_blob = f"{experiments_folder}/version_{model_version}/hparams.yaml"
    else:
        # Fetch the latest version
        versions = [d.name for d in list_blobs(bucket_name, prefix=f"{experiments_folder}/version_") if d.name.endswith('hparams.yaml')]
        if not versions:
            raise Exception("No hparams.yaml files found.")
        hparams_blob = sorted(versions, reverse=True)[0]
    
    local_hparams = "hparams.yaml"
    download_blob(bucket_name, hparams_blob, local_hparams)
    with open(local_hparams, 'r') as file:
        hparams = yaml.safe_load(file)
    return hparams

def load_model(bucket_name, experiments_folder, model_version=None):
    # Load hyperparameters
    hparams = read_hparams(bucket_name, experiments_folder, model_version)
    
    # Initialize the model
    model = GPTModel(
        embed_size=hparams['embed_size'],
        num_layers=hparams['num_layers'],
        heads=hparams['heads'],
        forward_expansion=hparams['forward_expansion'],
        dropout_rate=hparams['dropout_rate'],
        batch_size=hparams['batch_size'],
        vocab_size=hparams['vocab_size'],
        sequence_length=hparams['sequence_length'],
        max_epochs=hparams['max_epochs'],
        training_file_path=hparams['training_file_path'],
        validation_file_path=hparams['validation_file_path']
    )
    
    # Load the latest checkpoint
    checkpoint_path = get_latest_checkpoint(bucket_name, experiments_folder)
    checkpoint = torch.load(checkpoint_path, map_location=torch.device('cpu'))
    model.load_state_dict(checkpoint['state_dict'])
    model.eval()
    
    return model, hparams

def top_p_filtering(logits, top_p=0.9, filter_value=-float('Inf')):
    """ Filter a distribution of logits using nucleus (top-p) sampling """
    sorted_logits, sorted_indices = torch.sort(logits, descending=True)
    cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)

    # Remove tokens with cumulative probability above the threshold (nucleus)
    sorted_indices_to_remove = cumulative_probs > top_p
    # Shift the indices to the right to keep the first token above the threshold
    sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()
    sorted_indices_to_remove[..., 0] = 0

    indices_to_remove = sorted_indices[sorted_indices_to_remove]
    logits[indices_to_remove] = filter_value
    return logits

def generate_text(input_text, tokenizer, model, sequence_length=128, temperature=1.0, top_p=0.9):
    if not input_text.strip():
        raise ValueError("Input text is empty")

    # Tokenize the input text
    input_ids = tokenizer.encode(input_text)

    if not input_ids:
        raise ValueError("Input text could not be tokenized")

    # Convert to tensor and add batch dimension
    input_ids = torch.tensor([input_ids], dtype=torch.long)

    print(f"Encoded input ids: {input_ids}")  # Debug print

    # Generate text
    with torch.no_grad():
        for _ in range(sequence_length):
            outputs = model(input_ids)
            logits = outputs[0, -1, :] / temperature  # Select the logits for the last word in the sequence
            filtered_logits = top_p_filtering(logits, top_p=top_p)

            # Sample from the filtered distribution
            probabilities = F.softmax(filtered_logits, dim=-1)
            next_token_id = Categorical(probabilities).sample()

            # Stop generating if end-of-sequence token is produced
            if next_token_id.item() == tokenizer.eot_token:
                print("End of sequence token reached.")  # Debug print
                break

            # Add batch dimension to make it a 2D tensor
            next_token_id = next_token_id.unsqueeze(0).unsqueeze(0)  # Add two dimensions to make it a 2D tensor
            input_ids = torch.cat((input_ids, next_token_id), dim=-1)

    generated_text = tokenizer.decode(input_ids[0].tolist())
    print(f"Generated text: {generated_text}")  # Debug print
    return generated_text

def predict_model(input_text, bucket_name, experiments_folder, model_version=None):
    tokenizer = tiktoken.encoding_for_model("gpt2")  # Ensure this matches your model's vocabulary

    # Load model
    model, hparams = load_model(bucket_name, experiments_folder, model_version)
    sequence_length = hparams.get('sequence_length', 128)

    # Generate text using the trained model
    generated_text = generate_text(input_text, tokenizer, model, sequence_length=sequence_length)
    
    # Optionally, upload the generated text to GCS
    output_blob = f"{experiments_folder}/experiments_generated_text.txt"
    with open("generated_text.txt", "w", encoding='utf-8') as f:
        f.write(generated_text)
    upload_blob(bucket_name, "generated_text.txt", output_blob)
    
    return generated_text
# encode.py
import os
import json
import tiktoken

def restructure_data(input_file, output_file, context_window):
    with open(input_file, 'r', encoding='utf-8') as file:
        lines = file.readlines()

    all_text = "".join(lines)
    chunks = [all_text[i:i+context_window] for i in range(0, len(all_text), context_window)]

    with open(output_file, 'w', encoding='utf-8') as file:
        for idx, chunk in enumerate(chunks, start=1):  # Start the indexing from 1
            json_obj = {
                "id": idx,
                "text": chunk.strip(),
                "length": len(chunk.split()),  # Number of words in the chunk
                "ended": idx == len(chunks)  # True if it's the last chunk
            }
            file.write(json.dumps(json_obj) + '\n')

def encode_jsonl_file(input_file):
    # Initialize the tokenizer for GPT-2
    encoder = tiktoken.encoding_for_model("gpt2")

    # Construct output file name by replacing the extension with '.encoded.txt'
    base_name = os.path.splitext(input_file)[0]
    output_file = base_name + '.encoded.txt'

    with open(input_file, 'r', encoding='utf-8') as infile, \
         open(output_file, 'w', encoding='utf-8') as outfile:
        for line in infile:
            # Parse the JSON line
            data = json.loads(line)

            # Encode the text
            encoded_text = encoder.encode(data['text'])

            # Convert encoded tokens to a string and write to the output file
            outfile.write(' '.join(map(str, encoded_text)) + '\n')

    print(f"Encoded file saved as: {output_file}")

def encode_text_file(file_path, val_ratio=0.1, context_window=1024):
    structured_file = os.path.join(os.path.dirname(file_path), 'structured_data.txt')
    restructure_data(file_path, structured_file, context_window)

    encoder = tiktoken.encoding_for_model("gpt2")
    directory = os.path.dirname(structured_file)
    encoded_file = os.path.join(directory, 'encoded_data.txt')
    train_file = os.path.join(directory, 'training_data.txt')
    val_file = os.path.join(directory, 'validation_data.txt')

    with open(structured_file, 'r', encoding='utf-8') as file:
        sentences = file.readlines()

    with open(encoded_file, 'w', encoding='utf-8') as file:
        for sentence in sentences:
            tokens = encoder.encode(sentence.strip())
            token_str = ' '.join(map(str, tokens))
            file.write(token_str + '\n')

    format_and_split_data(encoded_file, train_file, val_file, val_ratio)

def format_and_split_data(file_path, train_file, val_file, val_ratio=0.1):
    with open(file_path, 'r', encoding='utf-8') as file:
        tokens = [' '.join(line.strip().split()) for line in file.readlines()]

    val_size = int(len(tokens) * val_ratio)
    train_tokens = tokens[:-val_size]
    val_tokens = tokens[-val_size:]

    with open(train_file, 'w', encoding='utf-8') as file:
        file.write('\n'.join(train_tokens) + '\n')
    with open(val_file, 'w', encoding='utf-8') as file:
        file.write('\n'.join(val_tokens) + '\n')

def find_vocab_size(file_path):
    max_token = 0
    with open(file_path, 'r', encoding='utf-8') as file:
        for line in file:
            tokens = [int(token) for token in line.strip().split()]
            max_token = max(max_token, max(tokens))
    return max_token + 1

# Example usage:
# encode_file('data/raw_data.txt', val_ratio=0.1, context_window=1024)
#vocab_size = find_vocab_size('data/svelte_docs/training_data.txt')
#print("Vocabulary Size:", vocab_size)# model.py
import torch
import lightning as L
import torch.nn as nn

from torch.nn import functional as F
from layers import GPTTransformerBlock
from util import sinusoidal_positional_encoding

class GPTModel(L.LightningModule):
    def __init__(self, embed_size, num_layers, heads, forward_expansion, dropout_rate,
                 vocab_size, batch_size, sequence_length, max_epochs,
                 dataset_length):
        super(GPTModel, self).__init__()
        self.save_hyperparameters()  # Save the model's hyperparameters
        self.dataset_length = dataset_length  # Use the passed dataset_length

        self.embed_size = embed_size
        self.num_layers = num_layers
        self.heads = heads
        self.forward_expansion = forward_expansion
        self.vocab_size = vocab_size
        self.batch_size = batch_size
        self.sequence_length = sequence_length
        self.max_epochs = max_epochs

        # Example input array (adjust the shape according to your model's input)
        self.example_input_array = torch.zeros((1, sequence_length), dtype=torch.long)

        self.embedding = nn.Embedding(self.vocab_size, self.embed_size)
        self.pos_embbedings = nn.Parameter(sinusoidal_positional_encoding(embed_size, max_len=sequence_length))

        self.layers = nn.ModuleList([
            GPTTransformerBlock(embed_size, heads, forward_expansion, dropout_rate)
            for _ in range(num_layers)
        ])
        self.output_layer = nn.Linear(embed_size, vocab_size, bias=False)
        self.output_layer.weight = self.embedding.weight

    def forward(self, x, mask=None):
        x = self.embedding(x)
        current_seq_length = x.size(1)
        x = x + self.pos_embbedings[:, :current_seq_length, :]
    
        # Transpose x to have shape (sequence_length, batch_size, embed_size)
        x = x.transpose(0, 1)

        # Adjust the mask for multi-head attention
        causal_mask = self.create_causal_mask(current_seq_length)
        if mask is not None:
            # Adjust the mask dimensions to match the causal mask
            mask = mask.unsqueeze(1).repeat(1, self.heads, 1, 1)
            mask = mask.bool()
            mask = mask | causal_mask
        else:
            mask = causal_mask

        for layer in self.layers:
            x = layer(x, mask=mask)  # Pass the mask to each layer

        x = self.output_layer(x)

        return x
    
    def create_causal_mask(self, size):
        device = next(self.parameters()).device  # Get the device from model parameters
        mask = torch.triu(torch.ones(size, size, device=device), diagonal=1).bool()
        return mask


        
    def masked_loss(self, outputs, targets, masks):
        # Flatten outputs and targets
        outputs_flat = outputs.view(-1, self.vocab_size)
        targets_flat = targets.view(-1)

        # Use masks to filter out loss from padding tokens
        mask = masks.view(-1) == 1  # Flatten and convert to boolean mask
        outputs_masked = outputs_flat[mask]
        targets_masked = targets_flat[mask]

        # Calculate cross-entropy loss only on non-padded tokens
        return F.cross_entropy(outputs_masked, targets_masked)

    def training_step(self, batch):
        inputs, targets, masks = batch
        outputs = self(inputs, mask=masks)
        loss = self.masked_loss(outputs, targets, masks)

        self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)
        return loss

    def validation_step(self, batch):
        inputs, targets, masks = batch
        outputs = self(inputs, mask=masks)
        loss = self.masked_loss(outputs, targets, masks)

        self.log('val_loss', loss, on_epoch=True, prog_bar=True, logger=True)
        return loss

    def configure_optimizers(self):
        optimizer = torch.optim.AdamW(self.parameters(), lr=1e-4, weight_decay=0.01)

        num_batches_per_epoch = self.dataset_length // self.batch_size
        if self.dataset_length % self.batch_size != 0:
            num_batches_per_epoch += 1

        total_steps = self.max_epochs * num_batches_per_epoch
        warmup_steps = int(0.1 * total_steps)  # Example: 10% of total steps for warmup

        scheduler = WarmupCosineLR(optimizer, warmup_steps=warmup_steps, total_steps=total_steps)

        return {
            'optimizer': optimizer,
            'lr_scheduler': {
                'scheduler': scheduler,
                'interval': 'step',  # 'step' means the scheduler step is called after every batch
            },
        }

class WarmupCosineLR(torch.optim.lr_scheduler._LRScheduler):
    def __init__(self, optimizer, warmup_steps, total_steps, min_lr=0.000001, last_epoch=-1):
        self.warmup_steps = warmup_steps
        self.total_steps = total_steps
        self.min_lr = min_lr
        super().__init__(optimizer, last_epoch)

    def get_lr(self):
        if self.last_epoch < self.warmup_steps:
            lr_scale = self.last_epoch / self.warmup_steps
        else:
            progress = (self.last_epoch - self.warmup_steps) / (self.total_steps - self.warmup_steps)
            lr_scale = 0.5 * (1.0 + torch.cos(torch.tensor(progress, device=self._get_device())))

        return [base_lr * lr_scale + self.min_lr for base_lr in self.base_lrs]

    def _get_device(self):
        return self.optimizer.param_groups[0]['params'][0].device


# dataset.py
import torch
import lightning as L
from torch.nn.utils.rnn import pad_sequence
from torch.utils.data import DataLoader, Dataset
import os
from gcs_utils import download_blob

# Collate function outside the dataset class
def collate_fn(batch):
    inputs, targets, masks = zip(*batch)
    inputs_padded = pad_sequence(inputs, batch_first=True, padding_value=0)
    targets_padded = pad_sequence(targets, batch_first=True, padding_value=0)
    masks_padded = pad_sequence(masks, batch_first=True, padding_value=0)  # Pad attention masks
    
    # Ensure masks_padded is boolean
    masks_padded = masks_padded.bool()
    return inputs_padded, targets_padded, masks_padded

class TokenizedTextDataset(Dataset):
    def __init__(self, file_path, sequence_length, padding_token=0, in_memory=True):
        self.file_path = file_path
        self.sequence_length = sequence_length
        self.padding_token = padding_token
        self.in_memory = in_memory
        self.data = []
        self.line_offsets = []

        if self.in_memory:
            self._load_dataset_into_memory()
        else:
            self._index_file_positions()

    def _load_dataset_into_memory(self):
        with open(self.file_path, 'r') as file:
            for line in file:
                self.data.append(line.strip())

    def _index_file_positions(self):
        with open(self.file_path, 'r') as file:
            offset = 0
            for line in file:
                self.line_offsets.append(offset)
                offset += len(line)

    def __len__(self):
        return len(self.data) if self.in_memory else len(self.line_offsets)

    def __getitem__(self, idx):
        if self.in_memory:
            line = self.data[idx]
        else:
            with open(self.file_path, 'r') as file:
                file.seek(self.line_offsets[idx])
                line = file.readline().strip()

        sequence = list(map(int, line.split()))

        # Padding or truncating the sequence
        if len(sequence) < self.sequence_length:
            sequence += [self.padding_token] * (self.sequence_length - len(sequence))
        else:
            sequence = sequence[:self.sequence_length]

        # Generate an attention mask for the sequence
        attention_mask = [1 if token != self.padding_token else 0 for token in sequence]

        input_sequence = torch.tensor(sequence[:-1], dtype=torch.long)
        target_sequence = torch.tensor(sequence[1:], dtype=torch.long)
        attention_mask = torch.tensor(attention_mask[:-1], dtype=torch.float)

        return input_sequence, target_sequence, attention_mask

class GPTDataModule(L.LightningDataModule):
    def __init__(self, train_blob_name, val_blob_name, bucket_name, batch_size=32, sequence_length=1024, local_data_dir='temp_data'):
        super().__init__()
        self.train_blob_name = train_blob_name
        self.val_blob_name = val_blob_name
        self.bucket_name = bucket_name
        self.batch_size = batch_size
        self.sequence_length = sequence_length
        self.local_data_dir = local_data_dir

    def setup(self, stage=None):
        """
        Downloads the training and validation data from GCS to a local directory.
        """
        if not os.path.exists(self.local_data_dir):
            os.makedirs(self.local_data_dir)

        # Download training data
        self.train_file = os.path.join(self.local_data_dir, os.path.basename(self.train_blob_name))
        download_blob(self.bucket_name, self.train_blob_name, self.train_file)

        # Download validation data
        self.val_file = os.path.join(self.local_data_dir, os.path.basename(self.val_blob_name))
        download_blob(self.bucket_name, self.val_blob_name, self.val_file)

        # Create dataset instances
        self.train_dataset = TokenizedTextDataset(self.train_file, self.sequence_length)
        self.val_dataset = TokenizedTextDataset(self.val_file, self.sequence_length)

    def train_dataloader(self):
        return DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True, collate_fn=collate_fn, pin_memory=True)

    def val_dataloader(self):
        return DataLoader(self.val_dataset, batch_size=self.batch_size, shuffle=False, collate_fn=collate_fn, pin_memory=True)

    def teardown(self, stage=None):
        """
        Cleans up the local data directory after training/testing.
        """
        if os.path.exists(self.local_data_dir):
            import shutil
            shutil.rmtree(self.local_data_dir)
# callbacks.py
from lightning.pytorch.callbacks import ModelCheckpoint, Callback
from gcs_utils import upload_blob
import threading
import time
from gcs_utils import upload_blob
import os

class GCSCheckpointCallback(ModelCheckpoint):
    def __init__(self, bucket_name, gcs_ckpt_path, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.bucket_name = bucket_name
        self.gcs_ckpt_path = gcs_ckpt_path

    def on_save_checkpoint(self, trainer, pl_module, checkpoint):
        # Call the superclass method to ensure checkpoint is saved
        super().on_save_checkpoint(trainer, pl_module, checkpoint)

        # Get the path of the last saved checkpoint
        last_ckpt_path = self.last_model_path

        if last_ckpt_path:
            # Construct the destination path in GCS
            filename = os.path.basename(last_ckpt_path)
            gcs_checkpoint_path = os.path.join(self.gcs_ckpt_path, filename).replace("\\", "/")

            # Upload the checkpoint to GCS
            upload_blob(self.bucket_name, last_ckpt_path, gcs_checkpoint_path)
            print(f"Uploaded checkpoint to gs://{self.bucket_name}/{gcs_checkpoint_path}")


class GCSTensorBoardLoggerCallback(Callback):
    def __init__(self, bucket_name, local_tb_log_dir, gcs_tb_log_dir, upload_interval=300):
        super().__init__()
        self.bucket_name = bucket_name
        self.local_tb_log_dir = local_tb_log_dir
        self.gcs_tb_log_dir = gcs_tb_log_dir
        self.upload_interval = upload_interval  # In seconds
        self.stop_event = threading.Event()
        self.thread = threading.Thread(target=self.upload_logs_periodically)
        self.thread.daemon = True  # Daemon thread will exit when main program exits

    def on_train_start(self, trainer, pl_module):
        # Start the upload thread
        self.thread.start()

    def on_train_end(self, trainer, pl_module):
        # Stop the upload thread
        self.stop_event.set()
        self.thread.join()
        # Perform a final upload to ensure all logs are uploaded
        self.upload_logs()

    def upload_logs_periodically(self):
        while not self.stop_event.is_set():
            self.upload_logs()
            time.sleep(self.upload_interval)

    def upload_logs(self):
        for root, dirs, files in os.walk(self.local_tb_log_dir):
            for file in files:
                local_file_path = os.path.join(root, file)
                relative_path = os.path.relpath(local_file_path, self.local_tb_log_dir)
                gcs_file_path = os.path.join(self.gcs_tb_log_dir, relative_path).replace("\\", "/")
                try:
                    upload_blob(self.bucket_name, local_file_path, gcs_file_path)
                except Exception as e:
                    print(f"Error uploading {local_file_path}: {e}")# main.py
import os
import argparse
import torch
import wandb
import datetime 
import yaml 

from encode import encode_text_file, encode_jsonl_file, restructure_data
from model import GPTModel
from predict import predict_model
from dataset import GPTDataModule  # Import the updated GPTDataModule
from gcs_utils import upload_blob
from callbacks import GCSCheckpointCallback, GCSTensorBoardLoggerCallback
from lightning.pytorch.callbacks import ModelCheckpoint
from lightning.pytorch import Trainer
from lightning.pytorch.loggers import TensorBoardLogger
from pytorch_lightning.loggers import WandbLogger

# Assuming the key is in the project root directory
os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = "zeti-nube-dev-key.json"

# Initalize WandB Project For Loggin
wandb.init(project='gpt')

def train_model(bucket_name, train_blob_name, val_blob_name):
    # Generate a unique timestamped directory name
    timestamp = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')
    experiment_dir = f'agi/experiments/{timestamp}/'

    # Initialize your data module
    data_module = GPTDataModule(
        bucket_name=bucket_name,
        train_blob_name=train_blob_name,
        val_blob_name=val_blob_name,
        batch_size=32,
        sequence_length=1024
    )
    data_module.setup()

    # Calculate dataset length
    dataset_length = len(data_module.train_dataset)

    # Initialize model with dataset_length
    model = GPTModel(
        embed_size=768,
        num_layers=12,
        heads=12,
        forward_expansion=4,
        dropout_rate=0.1,
        vocab_size=50233,
        batch_size=32,
        sequence_length=1024,
        max_epochs=10,
        dataset_length=dataset_length
    )

    # Define local directories with the timestamp
    local_checkpoint_dir = f'checkpoints/{timestamp}/'
    local_tb_log_dir = f'tb_logs/{timestamp}/'

    # Ensure the local directories exist
    os.makedirs(local_checkpoint_dir, exist_ok=True)
    os.makedirs(local_tb_log_dir, exist_ok=True)

    # **Save hyperparameters to a YAML file**
    hparams_file = os.path.join(local_checkpoint_dir, 'hparams.yaml')
    with open(hparams_file, 'w') as f:
        yaml.dump(model.hparams, f)

    # **Upload hparams file to GCS**
    gcs_hparams_path = f'{experiment_dir}checkpoints/hparams.yaml'
    upload_blob(bucket_name, hparams_file, gcs_hparams_path)
    print(f"Hyperparameters saved to gs://{bucket_name}/{gcs_hparams_path}")

    # Initialize the loggers with the new local directory
    tb_logger = TensorBoardLogger(local_tb_log_dir, name="gpt", log_graph=True)
    wandb_logger = WandbLogger(project='gpt')
    wandb_logger.experiment.config["batch_size"] = model.batch_size

    # Define the GCS paths using the experiment directory
    gcs_checkpoint_path = f'{experiment_dir}checkpoints/'
    gcs_tb_log_dir = f'{experiment_dir}tb_logs/'

    # Define the custom checkpoint callback
    checkpoint_callback = GCSCheckpointCallback(
        bucket_name=bucket_name,
        gcs_ckpt_path=gcs_checkpoint_path,
        dirpath=local_checkpoint_dir,
        filename='model-{epoch:02d}-{val_loss:.2f}',
        save_top_k=3,
        monitor='val_loss',
        mode='min'
    )

    # Define the custom TensorBoard logger callback
    tb_logger_callback = GCSTensorBoardLoggerCallback(
        bucket_name=bucket_name,
        local_tb_log_dir=local_tb_log_dir,
        gcs_tb_log_dir=gcs_tb_log_dir,
        upload_interval=300  # Adjust the interval as needed
    )

    # Initialize the Trainer with the custom callbacks
    trainer = Trainer(
        max_epochs=model.max_epochs,
        logger=[tb_logger, wandb_logger],
        devices=torch.cuda.device_count() if torch.cuda.is_available() else 1,
        accelerator="gpu" if torch.cuda.is_available() else 'auto',
        precision='16-mixed',
        callbacks=[checkpoint_callback, tb_logger_callback]
    )

    # Train the model
    trainer.fit(model, datamodule=data_module)
    tb_logger.save()
    wandb.finish()

    # Save and upload the final model (optional)
    local_model_path = 'final_model.pth'
    torch.save(model.state_dict(), local_model_path)
    gcs_model_path = f'{experiment_dir}final_model.pth'  # Use the experiment directory
    upload_blob(bucket_name, local_model_path, gcs_model_path)
    print(f"Model saved to gs://{bucket_name}/{gcs_model_path}")

    # Optionally, clean up local files
    # os.remove(local_model_path)
    # shutil.rmtree(local_checkpoint_dir)
    # shutil.rmtree(local_tb_log_dir)

def main(args):
    bucket_name = 'zdresearch'  # Define your bucket name
    experiments_folder = 'agi/experiments/'
    if args.command == 'encode':
        input_file = args.input_file if args.input_file else 'data/svelte_docs/raw_data.txt'
        encode_text_file(input_file)
        print(f"Encoded Tokens written")
    elif args.command == 'restructure':
        input_file = args.input_file if args.input_file else 'data/svelte_docs/raw_data.txt'
        restructure_data(input_file, 'data/svelte_docs/structured_data.txt', 128)
        print(f"Structured data written")
    elif args.command == 'encode_json':
        input_file = args.input_file if args.input_file else 'data/web_text/small-117M.valid.jsonl'
        encode_jsonl_file(input_file)
        print(f"Encoded JSONL Tokens written")
    elif args.command == 'train':
        bucket_name = 'zdresearch'  # Your bucket name
        train_blob = 'agi/svelte_docs/training_data.txt'
        val_blob = 'agi/svelte_docs/validation_data.txt'
        train_model(bucket_name, train_blob, val_blob)
        print("Training Complete")
    elif args.command == 'predict':
        input_text = "What is Svelte?"
        predict_model(input_text, bucket_name, experiments_folder)

    else:
        print("Invalid command")

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="AGI Model Operations")
    subparsers = parser.add_subparsers(dest='command', help='select operation')

    # Subparser for encoding
    parser_encode = subparsers.add_parser('encode', help='Encode text data to tokens')
    parser_encode.add_argument('input_file', type=str, nargs='?', default=None, help='Input file path')

    # Subparser for training
    parser_train = subparsers.add_parser('train', help='Train the GPT model')

    # Add predict command
    predict_parser = subparsers.add_parser('predict', help='Predict output for a given input text')

    # Subparser for restructuring
    parser_restructure = subparsers.add_parser('restructure', help='Restructure raw text data into structured format')
    parser_restructure.add_argument('input_file', type=str, nargs='?', default=None, help='Input file path for restructuring')

    # Subparser for encoding JSONL
    parser_encode_json = subparsers.add_parser('encode_json', help='Encode JSONL file')
    parser_encode_json.add_argument('input_file', type=str, nargs='?', default=None, help='Input JSONL file path for encoding')

    # Parse the arguments and call the main function
    args = parser.parse_args()
    main(args)
# layers.py
import torch
import math
import torch.nn as nn

class GPTTransformerBlock(nn.Module):
    def __init__(self, embed_size, heads, forward_expansion, dropout_rate):
        super(GPTTransformerBlock, self).__init__()
        self.attention = nn.MultiheadAttention(embed_dim=embed_size, num_heads=heads)
        self.dropout = nn.Dropout(dropout_rate)  # Dropout layer
        self.norm1 = nn.LayerNorm(embed_size)
        self.norm2 = nn.LayerNorm(embed_size)
        self.feed_forward = nn.Sequential(
            nn.Linear(embed_size, forward_expansion * embed_size),
            nn.GELU(),
            nn.Linear(forward_expansion * embed_size, embed_size)
        )

    def forward(self, x, mask=None):
        attention_output, _ = self.attention(x, x, x, attn_mask=mask)
        x = self.norm1(self.dropout(attention_output) + x)  # Apply dropout after attention
        forward_output = self.feed_forward(x)
        out = self.norm2(self.dropout(forward_output) + x)  # Apply dropout after feed-forward network

        return out
# gcs_utils.py
import os
from google.cloud import storage
from google.api_core.exceptions import GoogleAPIError

def initialize_storage_client():
    """
    Initializes and returns a Google Cloud Storage client.
    """
    try:
        client = storage.Client()
        return client
    except Exception as e:
        print(f"Error initializing Google Cloud Storage client: {e}")
        raise e

def download_blob(bucket_name, source_blob_name, destination_file_name):
    """
    Downloads a blob from the bucket.
    """
    client = initialize_storage_client()
    bucket = client.bucket(bucket_name)
    blob = bucket.blob(source_blob_name)
    try:
        blob.download_to_filename(destination_file_name)
        print(f"Downloaded {source_blob_name} to {destination_file_name}.")
    except GoogleAPIError as e:
        print(f"Error downloading blob: {e}")
        raise e

def upload_blob(bucket_name, source_file_name, destination_blob_name):
    """
    Uploads a file to the bucket.
    """
    client = initialize_storage_client()
    bucket = client.bucket(bucket_name)
    blob = bucket.blob(destination_blob_name)
    try:
        blob.upload_from_filename(source_file_name)
        print(f"Uploaded {source_file_name} to gs://{bucket_name}/{destination_blob_name}.")
    except GoogleAPIError as e:
        print(f"Error uploading blob: {e}")
        raise e

def list_blobs(bucket_name, prefix=None):
    """
    Lists all blobs in the bucket with an optional prefix.
    """
    client = initialize_storage_client()
    bucket = client.bucket(bucket_name)
    blobs = bucket.list_blobs(prefix=prefix)
    return blobs
<a href="https://colab.research.google.com/github/ZetiAi/agi/blob/master/agi.ipynb" target="_parent"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a>
Begin!
import time

from datetime import datetime

start_time = time.time()  # Start time

current_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")  # Current time in readable format

print(f"Run begins at {current_time}")
Download & Install Python 3.11
!sudo apt-get update

!sudo apt-get install -y make build-essential binfmt-support

!sudo add-apt-repository ppa:deadsnakes/ppa -y

!sudo apt install python3.11 -y

!python3.11 --version
Clone the Repo & Create Path to Directory
import getpass

import subprocess

import os



# Step 1: Prompt the user for their GitHub PAT securely

pat = getpass.getpass("Enter your GitHub Personal Access Token (PAT): ")



# Step 2: Construct the clone URL using the PAT

clone_url = f"https://{pat}@github.com/ZetiAi/agi.git"



# Step 3: Clone the repository without printing the PAT

# Suppress the output to ensure the PAT isn't logged

result = subprocess.run(

    ["git", "clone", clone_url],

    stdout=subprocess.PIPE,

    stderr=subprocess.PIPE,

    text=True

)



# Optional: Check if the clone was successful

if result.returncode == 0:

    print("Repository cloned successfully.")

else:

    print("Error cloning repository:")

    print(result.stderr)



# Step 4: Change directory to the cloned repository

%cd agi



# Step 5: Remove the PAT from the environment to prevent accidental exposure

del pat

del clone_url
Install Poetry

!sudo apt update

!sudo apt install pipx

!pipx ensurepath

!pipx install poetry
Train the Model
!/root/.local/bin/poetry env use 3.11
!/root/.local/bin/poetry install --no-root
!nvidia-smi
%reload_ext tensorboard

%tensorboard --logdir=/content/agi/tb_logs/ --port 6013
import time

!export WANDB_MODE=online

!wandb login

setup_time = time.time() - start_time

print(f"Setup time: {setup_time:.2f} seconds ({setup_time / 60:.2f} minutes)")

training_start_time = time.time()

!/root/.local/bin/poetry run python src/main.py train

training_time = time.time() - training_start_time

print(f"Training time: {training_time:.2f} seconds ({training_time / 60:.2f} minutes)")
!/root/.local/bin/poetry run python src/main.py predict
